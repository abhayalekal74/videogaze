# -*- coding: utf-8 -*-
"""Gaze-Following.ipynb

Automatically generated by Colaboratory.

Original file is located at
	https://colab.research.google.com/drive/1-mPyyqcK4EavvDTc8TPdRFbLwTJ_a1cW
"""

#!git clone https://github.com/recasens/Gaze-Following.git

#!mv Gaze-Following GazeFollowing

#!pip install face_recognition

#!wget http://videogazefollow.csail.mit.edu/downloads/model.pth.tar

import face_recognition
import cv2
import pylab
import imageio
from cv2 import CascadeClassifier
import sys
import argparse
import os
import shutil
import time
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.backends.cudnn as cudnn
import torch.optim
import torch.utils.data
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import torchvision.models as models
import numpy as np
from GazeFollowing.videogaze_model import VideoGaze
import cv2
import math
from sklearn import metrics
from mtcnn.mtcnn import MTCNN

#!pip install scikit-image

#Loading the model
model = VideoGaze(bs=32,side=20)
checkpoint = torch.load('model.pth.tar', map_location=torch.device('cpu'))
model.load_state_dict(checkpoint['state_dict'])
#model.cuda()
cudnn.benchmark = True

def download_video_frames(video_path):
	video_name = video_path[video_path.rindex('/') + 1: ]
	dest = video_name[:video_name.rindex('.')].replace('.', '-')
	os.system('sh download_video_frames.sh {} {} {}'.format(video_path, video_name, dest))
	return dest, 1

#!wget https://cloudfrontvideo.gradeup.co/recordings_mp4/grdplv-1574685706653.mp4

#!ffmpeg -i "grdplv-1574685706653.mp4" -f image2 "video-frame%05d.png"

#Reading input video
# video_name = 'https://github.com/ageitgey/face_recognition/raw/master/examples/hamilton_clip.mp4'
# video_name = 'Bank Foundation Course.mp4'
# video_name = "https://cleovod.akamaized.net/recordings/1bwhprsbbk/nhwhprrp1k/F20.mp4"
video_name = sys.argv[1]
dest, fps = download_video_frames(video_name)
#dest, fps = "grdplv-1574685706653", 1
# vid = imageio.get_reader(video_name, 'mp4')
# fps = vid.get_meta_data()['fps']
# print (vid, fps)
frame_list = []
for f in os.listdir(dest):
	frame_list.append(cv2.imread(os.path.join(dest, f)))
# try:
#	 for im in vid:
#		 frame_list.append(im)
# except RuntimeError as e:
#	 print ("Error", e)
# vid = cv2.VideoCapture(video_name)
# frame_list = []
# success = True
# while success:
#	 success, frame = vid.read()
#	 frame_list.append(frame)
"""
cap = Video(video_name)
fc = cap.frame_count()
for i in np.arange(fc):
	z = cap.get_index_frame(i)
	frame_list.append(z)
"""
print(len(frame_list))

trans = transforms.ToTensor()

#Output video
target_writer = imageio.get_writer(os.path.join(dest, 'output.mp4'), fps=fps)

#N corresponds to the number of frames in the window to explore
N = 25

#w_T corresponds to the number of frames to skip when sampling the target window
w_T = 40

w_fps = 1

target_frame = torch.FloatTensor(N,3,227,227)
#target_frame = target_frame.cuda()

face_frame = torch.FloatTensor(N,3,227,227)
#face_frame = target_frame.cuda()

eyes = torch.zeros(N,3)
#eyes = eyes.cuda()


#detector = MTCNN()
detector = CascadeClassifier('haarcascade_frontalface_default.xml')

for i in range(len(frame_list)):
	print('Processing of frame %d out of %d' % (i,len(frame_list)))
	try:
		#Avoid the problems with the video limit
		if i>w_fps*(N-1)//2 and i<(len(frame_list)-w_fps*(N-1)//2):
			#Reading the image 
			top=False
			im = frame_list[i]
			h,w,c = im.shape


			#Detecting the person inside the image
			#tmp_encodings = face_recognition.face_encodings(im)
			# results = face_recognition.compare_faces(tmp_encodings, p_encoding)
			#faces = detector.detect_faces(im)
			face_locations = face_recognition.face_locations(im)
			for id,face_local in enumerate(face_locations):
				(top, right, bottom, left) = face_local
			"""
			faces = detector.detectMultiScale(im)
			#face_locations = face_recognition.face_locations(im)
			#print (face_locations)
			for face in faces:
				# if results[id]==True:
				print (face)
				#top, right, bottom, left = face['box']
				x, y, w, h = face
				top, right, bottom, left = y, x + w, y + h, x
			"""

			#If detection, run the model
			if top:

				#Crop Face Image 
				crop_img = im[top:bottom, left:right]
				crop_img = cv2.resize(im,(227,227))

				#Resize Image   
				im = cv2.resize(im,(227,227))

				#Compute the center of the head and estimate the eyes location
				eyes[:,0] = (right+left)/(2*w)
				eyes[:,1] = (top+bottom)/(2*h)

				#Fill the tensors for the exploring window. Face and source frame are the same
				source_frame = trans(im).view(1,3,227,227)
				face_frame = trans(crop_img).view(1,3,227,227)
				for j in range(N-1):
					trans_im = trans(im).view(1,3,227,227)
					source_frame = torch.cat((source_frame,trans_im),0)
					crop_im = trans(crop_img).view(1,3,227,227)
					face_frame = torch.cat((face_frame,crop_im),0)

				#Fill the targets for the exploring window. 
				for j in range(N):
					target_im = frame_list[i+w_fps*(j-((N-1)//2))]
					target_im = cv2.resize(target_im,(227,227))
					target_im = trans(target_im)
					target_frame[j,:,:,:] = target_im
				
				#Run the model
				"""
				source_frame = source_frame.cuda(async=True)
				target_frame = target_frame.cuda(async=True)
				face_frame = face_frame.cuda(async=True)
				eyes = eyes.cuda(async=True)
				"""
				source_frame_var = torch.autograd.Variable(source_frame)
				target_frame_var = torch.autograd.Variable(target_frame)
				face_frame_var = torch.autograd.Variable(face_frame)
				eyes_var = torch.autograd.Variable(eyes)
				output,sigmoid= model(source_frame_var,target_frame_var,face_frame_var,eyes_var)

				#Recover the data from the variables
				sigmoid = sigmoid.data
				output = output.data

				#Pick the maximum value for the frame selection
				v,ids = torch.sort(sigmoid, dim=0, descending=True)
				index_target = ids[0,0]

				#Pick the frames corresponding to the maximum value
				target_im = frame_list[i+w_fps*(index_target-((N-1)//2))].copy()
				output_target = cv2.resize(output[index_target,:,:,:].view(20,20).cpu().numpy(),(200,200))
				
				#Compute the gaze location
				map = np.reshape(output_target,(200*200))

				int_class = np.argmax(map)
				x_class = int_class % 200
				y_class = (int_class-x_class)//200
				y_float = y_class/200.0
				x_float = x_class/200.0
				x_point = math.floor(x_float*w)
				y_point = math.floor(y_float*h)

				print (target_im.shape)

				#Prepare video output
				tim = cv2.circle(target_im,(x_point,y_point), 30, (255,0,0), -1)
				face_im = cv2.rectangle(frame_list[i].copy(), (left, top), (right, bottom), (0, 0, 255), 3)
				final_im = np.concatenate((face_im,tim),axis=1)
				print (final_im.shape)
				target_writer.append_data(final_im)
		else:
			print ("Out of limit")
	except KeyError as e:
		print ("Error", e)


target_writer.close()


